---
title: "SLV Assignment 2"
author: "Hidde, Ilya, Parisa, & Pim"
date: '2023-09-21'
output: html_document
---

# Prediction Model
```{r setup, include=FALSE,comment=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(mice)
library(ggplot2)
library(corrplot)
library(randomForest)
library(caret)
library(xgboost)
library(devtools)
source_url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")
library(doParallel)
library(ParBayesianOptimization)
```

### Part 1. Introduction

```{r, include=FALSE}
data <- read.csv("heartdisease.csv")
data %>% head
```

World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. [This research](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression) intends to pinpoint the most relevant/risk factors of heart disease. The data includes:

• Sex: male or female(Nominal)
• Age: Age of the patient;(Continuous)
• Current Smoker: whether or not the patient is a current smoker (Nominal)
• Cigs Per Day: the number of cigarettes that the person smoked on average in one day
• BP Meds: whether or not the patient was on blood pressure medication (Nominal)
• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)
• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)
• Diabetes: whether or not the patient had diabetes (Nominal)
• Tot Chol: total cholesterol level (Continuous)
• Sys BP: systolic blood pressure (Continuous)
• Dia BP: diastolic blood pressure (Continuous)
• BMI: Body Mass Index (Continuous)
• Heart Rate: heart rate (Continuous)
• Glucose: glucose level (Continuous)
• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”) 

This assignment is structured as follows: In the first part, we present our data. In part 2, we tidy, explore and describe our data, making it possible to further process it. In part 3, we provide basic prediction models, supported by the explanations, interpretations, and graphs. In part 4, We improve our models by making them more complex, which is also accompanied with a discussion. In the part 5, conclusions about predictions are given

### Part 2. Data wrangling
#### Data Type

Lets explore format of each column.
```{r}
str(data)
```

Not all columns were imported as data types that we expected but, so we need to change the type for some of the variables.
```{r}
#specifying factor variables
dataupd <- data %>%
mutate(male = as.factor(male),
       currentSmoker = as.factor(currentSmoker),
       education = as.factor(education),
       BPMeds = as.factor(BPMeds),
       prevalentStroke = as.factor(prevalentStroke),
       prevalentHyp = as.factor(prevalentHyp),
       diabetes = as.factor( diabetes),
       TenYearCHD = as.factor(TenYearCHD))
```

Now, they are specified in a more relevant manner.

#### Missing data
Since factors are specified correctly we are able to explore missing data patterns:
```{r}
md.pattern(dataupd)
```
The md.pattern shows that there are not many missing values in this dataset. PMeds, education, and glucose are the variables which have absent values the most often. It generally can be ignored since the observations in general are usually complete. We will, however, fill in missing values by simple imputation based on the data type.

Investigate missing values

We look since this variable has the most missing variables. We look at the distribution of the variable and the distribution of the variable when the missing values are removed. 
```{r}
R <- is.na(dataupd$glucose) 
histogram(~male|R, data=dataupd)
histogram(~age|R, data=dataupd)
histogram(~education|R, data=dataupd)
histogram(~cigsPerDay|R, data=dataupd)
```
There don't seem to be any differences in the distribution of the variables when the missing values are removed. Therefore, we assume that the missing values are missing at random or missing completely at random.

Imputing values
```{r, cache=TRUE}
imp <- mice(dataupd, m = 1, maxit = 1,  seed = 123, print = F)
plot(imp)
imp$meth
stripplot(imp)
```
No patterns of non-convergence were found. The stripplot shows that the imputed values are within the range of the observed values. In this assignment we will the non-imputed data, since there are no signs of non-convergence and the imputed values are within the range of the observed values.

#### normalize data to simplify model interpretation.
```{r}
#specify function for normalisation
normalise <- function(x) (x-min(x))/(max(x)-min(x))

# Define a function to standardize (z-score normalize) a numeric vector
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Deleting all missing values is better than simple imputation, as your underestimating the uncertainty, does not preserve the variability in the data.
data.deletion <- na.omit(data) 

data_normalised <- data.deletion %>%
  mutate_if(is.integer, as.numeric) %>%
  mutate(across(c(education, TenYearCHD, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes, male), as.factor)) %>%
  mutate_if(is.numeric, normalise)

# Apply z-score standardization to all numeric columns
data_standardized <- data.deletion %>%
  mutate_if(is.integer, as.numeric) %>%
  mutate(across(c(education, TenYearCHD, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes, male), as.factor)) %>%
  mutate_if(is.numeric, standardize)
```


#### Visualize data
```{r}
data_standardized %>%
  pivot_longer(cols = -c(TenYearCHD, male, education, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes), names_to = "name", values_to = "value") %>%
  ggplot(aes(x = value, color = as.factor(TenYearCHD), fill = as.factor(TenYearCHD))) +
  geom_boxplot(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()

data_standardized %>%
  pivot_longer(cols = -c(TenYearCHD, male, education, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes), names_to = "name", values_to = "value") %>%
  ggplot(aes(x = value, col = as.factor(TenYearCHD), fill = as.factor(TenYearCHD))) +
  geom_density(alpha = 0.8) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()
```


### Part 3. Basic Predictive Modelling
#### simple example of a logistic regression model
```{r}
mod1 <- glm(TenYearCHD ~ ., data = data_standardized, family = "binomial")
summary(mod1)
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Train 80% and test 20%
train_proportion <- 0.8

# Create an index for splitting the data
train_index <- createDataPartition(data_standardized$TenYearCHD, p = train_proportion, list = FALSE)

# Split the data into training and test sets
train_data <- data_standardized[train_index, ]
test_data <- data_standardized[-train_index, ]
```


```{r}
# Cross-validation
cv <- trainControl(method = "cv", number = 10)  # 5-fold cross-validation

# Create the logistic regression model with cross-validation
model_logistic <- train(
  TenYearCHD ~ .,
  data = train_data,
  method = "glm", 
  family = "binomial",
  trControl = cv
)

# Print the model results
print(model_logistic)

# Extract and print the coefficients
coef_summary <- summary(model_logistic$finalModel)
print(coef_summary)
```

```{r}
# Make predictions on the test data using the trained logistic regression model
test_predictions <- predict(model_logistic, newdata = test_data)

# Confusion matrix test
test_confusion_matrix <- confusionMatrix(test_predictions, test_data$TenYearCHD, positive = "1")

# Confusion matrix on the test data
print(test_confusion_matrix)
```
Accuracy:
The model has an accuracy of 85.34%, which means that overall, it correctly predicted the outcome for 85.34% of the test set.
TN = 718: The number of instances correctly predicted as class 0 (no 10-year risk of CHD).
FP = 124: The number of instances incorrectly predicted as class 0.
FN = 0: The number of instances incorrectly predicted as class 1 (meaning the model did not incorrectly predict any true class 0 instances as class 1).
TP = 4: The number of instances correctly predicted as class 1 (10-year risk of CHD).

### Part 4. A more complex model
```{r}
train_x <- model.matrix(TenYearCHD ~ ., train_data)[,-1]
train_y <- as.numeric(train_data$TenYearCHD) - 1
test_x <- model.matrix(TenYearCHD ~ ., test_data)[,-1]
test_y <- as.numeric(test_data$TenYearCHD) - 1
```

```{r}
params <- list(max_depth = 6,
               eta = 0.3,
               gamma = 0,
               min_child_weight = 1,
               subsample = 1,
               booster = "gbtree",
               objective = "binary:logistic",
               eval_metric = "auc",
               verbosity = 0)

xgbCV <- xgb.cv(params = params,
                data = train_x,
                label = train_y, 
                nrounds = 200,
                prediction = TRUE,
                showsd = TRUE,
                early_stopping_rounds = 20,
                maximize = TRUE,
                nfold = 20,
                stratified = TRUE)

numrounds <- min(which(xgbCV$evaluation_log$test_auc_mean == 
                         max(xgbCV$evaluation_log$test_auc_mean)))

fit <- xgboost(params = params,
               data = train_x,
               label = train_y,
               nrounds = numrounds)

pred.xgb <- predict(fit, test_x , type = "response")
ROCpred.xgb <- prediction(as.numeric(pred.xgb), as.numeric(test_y))
auc.xgb <- performance(ROCpred.xgb, measure = "auc")
auc <- auc.xgb@y.values[[1]]
names(auc) <- c("XGBoost AUC")
auc

ROCperf.xgb <- performance(ROCpred.xgb, 'tpr','fpr')
df_ROC.xgb <- data.frame(FalsePositive = c(ROCperf.xgb@x.values[[1]]),
                         TruePositive = c(ROCperf.xgb@y.values[[1]]))
```

```{r}
set.seed(123)
scoring_function <- function(
  eta, gamma, max_depth, min_child_weight, subsample, nfold) {
  dtrain <- xgb.DMatrix(train_x, label = train_y, missing = NA)

  pars <- list(
    eta = eta,
    gamma = gamma,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    subsample = subsample,

    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "auc",
    verbosity = 0
  )
  
  xgbcv <- xgb.cv(
    params = pars,
    data = dtrain,
    nfold = nfold,
    nrounds = 100,
    prediction = TRUE,
    showsd = TRUE,
    early_stopping_rounds = 10,
    maximize = TRUE,
    stratified = TRUE
  )
  
  # required by the package, the output must be a list
  # with at least one element of "Score", the measure to optimize
  # Score must start with capital S
  # For this case, we also report the best num of iteration
  return(
    list(
      Score = max(xgbcv$evaluation_log$test_auc_mean),
      nrounds = xgbcv$best_iteration
    )
  )
}

bounds <- list(
  eta = c(0, 1),
  gamma =c(0, 100),
  max_depth = c(2L, 10L), # L means integers
  min_child_weight = c(1, 25),
  subsample = c(0.25, 1),
  nfold = c(3L, 10L)
)


opt_obj <- bayesOpt(
  FUN = scoring_function,
  bounds = bounds,
  initPoints = 14,
  iters.n = 10)

# 10-10: 0.6939702 
# 14-10: 0.7127523 


```

```{r}
# take the optimal parameters for xgboost()
params <- list(eta = getBestPars(opt_obj)[1],
               gamma = getBestPars(opt_obj)[2],
               max_depth = getBestPars(opt_obj)[3],
               min_child_weight = getBestPars(opt_obj)[4],
               subsample = getBestPars(opt_obj)[5],
               nfold = getBestPars(opt_obj)[6],
               objective = "binary:logistic")

# the numrounds which gives the max Score (auc)
numrounds <- opt_obj$scoreSummary$nrounds[
  which(opt_obj$scoreSummary$Score
        == max(opt_obj$scoreSummary$Score))]

fit_tuned <- xgboost(params = params,
                     data = train_x,
                     label = train_y,
                     nrounds = numrounds,
                     eval_metric = "auc")
```

```{r}
# Usual AUC calculation
pred.xgb.tuned <- predict(fit_tuned, test_x, type = "response")
ROCpred.xgb.tuned <- prediction(as.numeric(pred.xgb.tuned), as.numeric(test_y))
auc.xgb.tuned <- performance(ROCpred.xgb.tuned, measure = "auc")
auc <- auc.xgb.tuned@y.values[[1]]
names(auc) <- c("XGBoost AUC Tuned")
auc
```

```{r}
ROCperf.xgb.tuned <- performance(ROCpred.xgb.tuned, 'tpr','fpr')
df_ROC.xgb_tune <- data.frame(FalsePositive = c(ROCperf.xgb.tuned@x.values[[1]]),
                              TruePositive = c(ROCperf.xgb.tuned@y.values[[1]]))

ggplot() +
  geom_line(data = df_ROC.xgb, aes(x = FalsePositive,
                                   y = TruePositive,
                                   color = "XGBoost:  0.6853251  ")) + 
  geom_line(data = df_ROC.xgb_tune, aes(x = FalsePositive,
                                        y = TruePositive,
                                        color = "XGBoost Tuned: 0.7127523   ")) + 
  geom_abline(slope = 1) + ggtitle("ROC Curves across models")
```

```{r}
pred <- tibble(TenYearCHD = predict(fit_tuned, newdata = train_x)) %>%
  mutate(TenYearCHD = factor(ifelse(TenYearCHD < 0.5, 1, 2),
                          labels = c("0", "1")))

table(pred$TenYearCHD, train_data$TenYearCHD)
shap_results <- shap.score.rank(fit_tuned,
                                X_train = train_x,
                                shap_approx = F)

var_importance(shap_results)
shap_long <- shap.prep(shap = shap_results,
                       X_train = train_x)

plot.shap.summary(shap_long)
xgb.plot.shap(train_x, features = colnames(train_x), model = fit_tuned, n_col = 3)
```

```{r}
logistic_test  <- predict(model_logistic, newdata = test_data)
xgb_normal <- predict(fit, newdata = test_x) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("0", "1"))
xgb_tuned <- predict(fit_tuned, newdata = test_x) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("0", "1"))

list(logistic_test, 
     xgb_normal,
     xgb_tuned) %>%
  map(~ confusionMatrix(.x, test_data$TenYearCHD, positive = "1"))
```


### Part 5. explain which method you use (regression/classification and what exactly)
we are using two different methods for predictive modeling: logistic regression and random forest. 

Logistic Regression
Method Type: Classification:
Logistic regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (where there are only two possible outcomes). In our case, the outcome is the 10-year risk of coronary heart disease (CHD), which is binary ("1" indicating "Yes", "0" indicating "No"). Logistic regression is used here to model the probability that a given input point belongs to the category of having a risk of CHD.
In the code, we create a logistic regression model (mod1) using the glm() function with the family set to "binomial", which indicates a logistic regression. we model the TenYearCHD as a function of several predictor variables including male, age, currentSmoker, diabetes, BMI, and glucose.

Random Forest
Method Type: Classification
Random forest is an ensemble learning method for classification (and regression) that works by constructing a multitude of decision trees at training time and outputting the mode of the classes (majority vote) for classification. 
In the code, a random forest model (model_rf) is created using the train() function from the caret package with the method set to "rf". This model also aims to predict the TenYearCHD outcome based on the same set of predictors as the logistic regression model. Random forests typically perform well for a wide range of data types without the need for extensive data preprocessing like normalization or scaling.

### Part 6. interpret the parameters of your method, if applicable.if no parameters, interpret the contribution of the features have to the model.

The random forests provide measures of feature importance, which indicate how much each feature contributes to the prediction. 
If a feature like age has a high importance score, it suggests that age is a strong predictor in determining the risk of coronary heart disease across the trees in the random forest.
For other features such as currentSmoker or BMI, their importance scores reflect their relative contribution to the model's predictions. A lower score would indicate that the feature is less influential.
