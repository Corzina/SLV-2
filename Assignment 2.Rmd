---
title: "SLV Assignment 2"
author: "Hidde, Ilya, Parisa, & Pim"
date: '2023-09-21'
output: html_document
---

# Prediction Model
```{r setup, include=FALSE,comment=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(mice)
library(ggplot2)
library(corrplot)
library(randomForest)
library(caret)
library(xgboost)
library(devtools)
source_url("https://github.com/pablo14/shap-values/blob/master/shap.R?raw=TRUE")
library(doParallel)
library(ROCR)
library(ParBayesianOptimization)
```

### Part 1. Introduction

```{r, include=FALSE}
data <- read.csv("heartdisease.csv")
data %>% head
```

World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. [This research](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression) intends to pinpoint the most relevant/risk factors of heart disease. The data includes:

• Sex: male or female(Nominal)
• Age: Age of the patient;(Continuous)
• Current Smoker: whether or not the patient is a current smoker (Nominal)
• Cigs Per Day: the number of cigarettes that the person smoked on average in one day
• BP Meds: whether or not the patient was on blood pressure medication (Nominal)
• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)
• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)
• Diabetes: whether or not the patient had diabetes (Nominal)
• Tot Chol: total cholesterol level (Continuous)
• Sys BP: systolic blood pressure (Continuous)
• Dia BP: diastolic blood pressure (Continuous)
• BMI: Body Mass Index (Continuous)
• Heart Rate: heart rate (Continuous)
• Glucose: glucose level (Continuous)
• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”) 

This assignment is structured as follows: In the first part, we present our data. In part 2, we tidy, explore and describe our data, making it possible to further process it. In part 3, we provide basic prediction models, supported by the explanations, interpretations, and graphs. In part 4, We improve our models by making them more complex, which is also accompanied with a discussion. In the part 5, conclusions about predictions are given

### Part 2. Data wrangling
#### Data Type

Lets explore format of each column.
```{r}
str(data)
```

Not all columns were imported as data types that we expected but, so we need to change the type for some of the variables.
```{r}
#specifying factor variables
dataupd <- data %>%
mutate(male = as.factor(male),
       currentSmoker = as.factor(currentSmoker),
       education = as.factor(education),
       BPMeds = as.factor(BPMeds),
       prevalentStroke = as.factor(prevalentStroke),
       prevalentHyp = as.factor(prevalentHyp),
       diabetes = as.factor( diabetes),
       TenYearCHD = as.factor(TenYearCHD))

str(dataupd)
```

Now, they are specified in a more relevant manner.

#### Missing data

Since factors are specified correctly we are able to explore missing data patterns:
```{r}
md.pattern(dataupd)
```
The md.pattern shows that there are not many missing values in this dataset. PMeds, education, and glucose are the variables which have absent values the most often. It generally can be ignored since the observations in general are usually complete. We will, however, fill in missing values by simple imputation based on the data type.

Investigate missing values

We look since this variable has the most missing variables. We look at the distribution of the variable and the distribution of the variable when the missing values are removed. 
```{r}
R <- is.na(dataupd$glucose) 
histogram(~male|R, data=dataupd)
histogram(~age|R, data=dataupd)
histogram(~education|R, data=dataupd)
histogram(~cigsPerDay|R, data=dataupd)
```
There don't seem to be any differences in the distribution of the variables when the missing values are removed. Therefore, we assume that the missing values are missing at random or missing completely at random.

Imputing values
```{r, cache=TRUE}
imp <- mice(dataupd, m = 1, maxit = 1,  seed = 123, print = F)
plot(imp)
imp$meth
stripplot(imp)
```
No patterns of non-convergence were found. The stripplot shows that the imputed values are within the range of the observed values. In this assignment we will the non-imputed data, since there are no signs of non-convergence and the imputed values are within the range of the observed values.

#### normalize data to simplify model interpretation.
```{r}
#specify function for normalisation
normalise <- function(x) (x-min(x))/(max(x)-min(x))

# Define a function to standardize (z-score normalize) a numeric vector
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Deleting all missing values is better than simple imputation, as your underestimating the uncertainty, does not preserve the variability in the data.
data.deletion <- na.omit(data) 

data_normalised <- data.deletion %>%
  mutate_if(is.integer, as.numeric) %>%
  mutate(across(c(education, TenYearCHD, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes, male), as.factor)) %>%
  mutate_if(is.numeric, normalise)

# Apply z-score standardization to all numeric columns
data_standardized <- data.deletion %>%
  mutate_if(is.integer, as.numeric) %>%
  mutate(across(c(education, TenYearCHD, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes, male), as.factor)) %>%
  mutate_if(is.numeric, standardize)
```


#### Visualize data
```{r}
data_standardized %>%
  pivot_longer(cols = -c(TenYearCHD, male, education, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes), names_to = "name", values_to = "value") %>%
  ggplot(aes(x = value, color = as.factor(TenYearCHD), fill = as.factor(TenYearCHD))) +
  geom_boxplot(alpha = 0.5) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()

data_standardized %>%
  pivot_longer(cols = -c(TenYearCHD, male, education, currentSmoker, BPMeds, prevalentStroke, prevalentHyp, diabetes), names_to = "name", values_to = "value") %>%
  ggplot(aes(x = value, col = as.factor(TenYearCHD), fill = as.factor(TenYearCHD))) +
  geom_density(alpha = 0.5) +
  facet_wrap(~name, scales = "free") +
  scale_color_brewer(palette = "Paired") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal()
```

### Part 3. Basic Predictive Modelling
#### simple example of a logistic regression model

```{r}
mod1 <- glm(TenYearCHD ~ ., data = data_standardized, family = "binomial")
summary(mod1)
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Train 80% and test 20%
train_proportion <- 0.8

# Create an index for splitting the data
train_index <- createDataPartition(data_standardized$TenYearCHD, p = train_proportion, list = FALSE)

# Split the data into training and test sets
train_data <- data_standardized[train_index, ]
test_data <- data_standardized[-train_index, ]
```


```{r}
# Cross-validation
cv <- trainControl(method = "cv", number = 10)  # 5-fold cross-validation

# Create the logistic regression model with cross-validation
model_logistic <- train(
  TenYearCHD ~ .,
  data = train_data,
  method = "glm", 
  family = "binomial",
  trControl = cv
)

# Print the model results
print(model_logistic)

# Extract and print the coefficients
coef_summary <- summary(model_logistic$finalModel)
print(coef_summary)
```


```{r}
# Make predictions on the test data using the trained logistic regression model
test_predictions <- predict(model_logistic, newdata = test_data)

# Confusion matrix test
test_confusion_matrix <- confusionMatrix(test_predictions, test_data$TenYearCHD, positive = "1")

# Confusion matrix on the test data
print(test_confusion_matrix)
```

Accuracy:
The model has an accuracy of 85.34%, which means that overall, it correctly predicted the outcome for 85.34% of the test set.
TN = 718: The number of instances correctly predicted as class 0 (no 10-year risk of CHD).
FP = 124: The number of instances incorrectly predicted as class 0.
FN = 0: The number of instances incorrectly predicted as class 1 (meaning the model did not incorrectly predict any true class 0 instances as class 1).
TP = 4: The number of instances correctly predicted as class 1 (10-year risk of CHD).

### Part 4. A more complex model

For the first step, we create design matrices for train and test dataframes, storing Y separately.
```{r}
train_x <- model.matrix(TenYearCHD ~ ., train_data)[,-1]
train_y <- as.numeric(train_data$TenYearCHD) - 1
test_x <- model.matrix(TenYearCHD ~ ., test_data)[,-1]
test_y <- as.numeric(test_data$TenYearCHD) - 1
```

Then, we fit gradient boosting model. Preliminary, cross-validated XGBoost is used to find optimal number of rounds.
```{r}
# Set hyperparameter values for XGBoost
params <- list(max_depth = 6,                  
               eta = 0.3,                    
               gamma = 0,                      
               min_child_weight = 1,           
               subsample = 1,                  # Subsample ratio of the training instances
               booster = "gbtree",             # Tree-based models
               objective = "binary:logistic",  # Binary logistic regression for classification
               eval_metric = "auc",            # Evaluation metric - Area Under the Curve (AUC)
               verbosity = 0)                  

# Perform cross-validated XGBoost to find optimal number of rounds
xgbCV <- xgb.cv(params = params,              
                data = train_x,                
                label = train_y,               
                nrounds = 200,                 
                prediction = TRUE,             
                showsd = TRUE,                 # Show standard deviation of AUC scores
                early_stopping_rounds = 20,    # Stop if performance doesn't improve for 20 rounds
                maximize = TRUE,               # Maximize the evaluation metric (AUC)
                nfold = 20,                    # Number of folds for cross-validation
                stratified = TRUE)             # Use stratified sampling for cross-validation

# Find the optimal number of rounds based on the maximum AUC score
numrounds <- min(which(xgbCV$evaluation_log$test_auc_mean == 
                         max(xgbCV$evaluation_log$test_auc_mean)))
numrounds

# Train the final XGBoost model using the optimal number of rounds
fit <- xgboost(params = params,                # XGBoost parameters
               data = train_x,                 # Training data features
               label = train_y,                # Training data labels
               nrounds = numrounds)            # Number of boosting rounds

# Make predictions on the test set using the trained XGBoost model
pred.xgb <- predict(fit, test_x , type = "response")

# Create a prediction object for ROC analysis
ROCpred.xgb <- ROCR::prediction(as.numeric(pred.xgb), as.numeric(test_y))

# Calculate the Area Under the Curve (AUC) for the XGBoost model
auc.xgb <- performance(ROCpred.xgb, measure = "auc")

# Extract the AUC value from the result
auc <- auc.xgb@y.values[[1]]

# Rename the AUC value for clarity
names(auc) <- c("XGBoost AUC")
auc

# Create a performance object for the ROC curve using the XGBoost predictions
ROCperf.xgb <- performance(ROCpred.xgb, 'tpr','fpr')

# Extract True Positive Rate (TPR) and False Positive Rate (FPR) values from the performance object
df_ROC.xgb <- data.frame(FalsePositive = c(ROCperf.xgb@x.values[[1]]),
                         TruePositive = c(ROCperf.xgb@y.values[[1]]))

```

This model prediction qualities can be improved, if we find optimal parameter values, which we do in a chunk below.
```{r}
set.seed(123)

# Specifying a function which finds the optimal values for xgboost() parameters
scoring_function <- function(
  eta, gamma, max_depth, min_child_weight, subsample, nfold) {
  dtrain <- xgb.DMatrix(train_x, label = train_y, missing = NA)

  pars <- list(
    eta = eta,
    gamma = gamma,
    max_depth = max_depth,
    min_child_weight = min_child_weight,
    subsample = subsample,
    booster = "gbtree",
    objective = "binary:logistic",
    eval_metric = "auc",
    verbosity = 0
  )
  
  xgbcv <- xgb.cv(
    params = pars,
    data = dtrain,
    nfold = nfold,
    nrounds = 100,
    prediction = TRUE,
    showsd = TRUE,
    early_stopping_rounds = 10,
    maximize = TRUE,
    stratified = TRUE
  )
  
  # required by the package, the output must be a list
  # with at least one element of "Score", the measure to optimize
  # Score must start with capital S
  # For this case, we also report the best num of iteration
  return(
    list(
      Score = max(xgbcv$evaluation_log$test_auc_mean),
      nrounds = xgbcv$best_iteration
    )
  )
}

bounds <- list(
  eta = c(0, 1),
  gamma =c(0, 100),
  max_depth = c(2L, 10L), # L means integers
  min_child_weight = c(1, 25),
  subsample = c(0.25, 1),
  nfold = c(3L, 10L)
)


opt_obj <- bayesOpt(
  FUN = scoring_function,
  bounds = bounds,
  initPoints = 14,
  iters.n = 10)

# 10-10: 0.6939702 
# 14-10: 0.7127523 
```

At the next step, we run tuned model with optimal parameters & calculate AUC for model quality estimation
```{r}
# take the optimal parameters for xgboost()
params <- list(eta = getBestPars(opt_obj)[1],
               gamma = getBestPars(opt_obj)[2],
               max_depth = getBestPars(opt_obj)[3],
               min_child_weight = getBestPars(opt_obj)[4],
               subsample = getBestPars(opt_obj)[5],
               nfold = getBestPars(opt_obj)[6],
               objective = "binary:logistic")

# the numrounds which gives the max Score (auc)
numrounds <- opt_obj$scoreSummary$nrounds[
  which(opt_obj$scoreSummary$Score
        == max(opt_obj$scoreSummary$Score))]

# Running the model with updated parameters
fit_tuned <- xgboost(params = params,
                     data = train_x,
                     label = train_y,
                     nrounds = numrounds,
                     eval_metric = "auc")
```

```{r}
# Usual AUC calculation
pred.xgb.tuned <- predict(fit_tuned, test_x, type = "response")
ROCpred.xgb.tuned <- prediction(as.numeric(pred.xgb.tuned), as.numeric(test_y))
auc.xgb.tuned <- performance(ROCpred.xgb.tuned, measure = "auc")
auctuned <- auc.xgb.tuned@y.values[[1]]
names(auctuned) <- c("XGBoost AUC Tuned")
auctuned
```

Now, when we can proceed to compare these models in terms of Sensitivity, Specificity and AUC.

```{r}
# Save output in a different format
ROCperf.xgb.tuned <- performance(ROCpred.xgb.tuned, 'tpr','fpr')
df_ROC.xgb_tune <- data.frame(FalsePositive = c(ROCperf.xgb.tuned@x.values[[1]]),
                              TruePositive = c(ROCperf.xgb.tuned@y.values[[1]]))

# Building a ROC curve comparison graph for XGBoost models with default/tuned parameters
ggplot() +
  geom_line(data = df_ROC.xgb, aes(x = FalsePositive,
                                   y = TruePositive,
                                   color = "XGBoost")) + 
  geom_line(data = df_ROC.xgb_tune, aes(x = FalsePositive,
                                        y = TruePositive,
                                        color = "XGBoost Tuned")) + 
  geom_abline(slope = 1) + ggtitle("ROC Curves across models") + 
  labs(x = "specificity", y="sensitivity")
```
As expected, tuned model performs slightly better - it has a larger AOC value. Also, for the most possible points in sensitivity-specificity tradeoff, tuned model end up with better, more accurate results.

Now, we continue exploring the impact on the model output made by different parameters by calculating SHAP values & marginal relationships of the SHAP values with the predictors.
```{r}
# Generate predictions using the tuned XGBoost model on the training data
pred <- tibble(TenYearCHD = predict(fit_tuned, newdata = train_x)) %>%
               mutate(TenYearCHD = factor(ifelse(TenYearCHD < 0.5, 1, 2),  # Convert pred probs to binary 
               labels = c("0", "1")))

# Create a confusion matrix to evaluate the model's performance on the training data
table(pred$TenYearCHD, train_data$TenYearCHD)

# Calculate SHAP (SHapley Additive exPlanations) values for feature importance
shap_results <- shap.score.rank(fit_tuned,
                                X_train = train_x,
                                shap_approx = F)

# Display variable importance using SHAP values
var_importance(shap_results)

# Prepare SHAP values for plotting
shap_long <- shap.prep(shap = shap_results,
                       X_train = train_x)

# Plot a summary of SHAP values
plot.shap.summary(shap_long)

# Generate SHAP summary plot using XGBoost-specific function
xgb.plot.shap(train_x, features = colnames(train_x), model = fit_tuned, n_col = 3)
```
This graphs can be interpreted in the following manner: The first graph shows that age and systolic pressure are the most important factors. The second specifies that, for example,that those with a high value forage have a higher probability of being diseased. The third ones describes precisely the relationship within a variable and the SHAP values.

As a final step, we provide summary for each model and compare quality metrics. 
```{r}
# prepare confusion matrices objects
logistic_test  <- predict(model_logistic, newdata = test_data)
xgb_normal <- predict(fit, newdata = test_x) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("0", "1"))
xgb_tuned <- predict(fit_tuned, newdata = test_x) %>%
  factor(x = ifelse(. < 0.5, 1, 2), levels = c(1,2), labels = c("0", "1"))

# show Confusion Matrix and Statistics for all models simultaneously
list(logistic_test, 
     xgb_normal,
     xgb_tuned) %>%
  map(~ confusionMatrix(.x, test_data$TenYearCHD, positive = "1"))
```

```{r}
cm1 <- confusionMatrix(logistic_test, test_data$TenYearCHD, positive = "1")
cm2 <- confusionMatrix(xgb_normal, test_data$TenYearCHD, positive = "1")
cm3 <- confusionMatrix(xgb_tuned, test_data$TenYearCHD, positive = "1")

# display confusion matrices in visual manner
df_cm1 <- as.data.frame(as.table(cm1))
df_cm2 <- as.data.frame(as.table(cm2))
df_cm3 <- as.data.frame(as.table(cm3))

# Combine data frames
df_combined <- rbind(cbind(df_cm1, Matrix = "Logistic Confusion Matrix"), 
                     cbind(df_cm2, Matrix = "XGB Confusion Matrix"),
                     cbind(df_cm3, Matrix = "XGB tuned Confusion Matrix"))

# Create a bar plot using ggplot2
ggplot(df_combined, aes(x = Prediction, y = Freq, fill = Reference)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~Matrix, scales = "free") +
  labs(title = "Comparison of Confusion Matrices",
       x = "Predicted Values", y = "Frequency", fill = "True Values") +
  scale_fill_manual(values = c("lightblue", "darkblue")) +
  theme_minimal()
```

### Part 5. explain which method you use (regression/classification and what exactly)

we are using two different classification methods for modeling: logistic regression and gradient boosting. 

Logistic Regression

A classical logistic regression, allows us to classify observations, based on set of predictors. In our case, the outcome is the 10-year risk of coronary heart disease (CHD), which is binary ("1" indicating "Yes", "0" indicating "No"). Logistic regression is used here to model the probability that a given input point belongs to the category of having a risk of CHD. In the code, we create a logistic regression model (mod1) using the glm() function with the family set to "binomial", which indicates a logistic regression. we model the TenYearCHD as a function of several predictor variables including male, age, currentSmoker, diabetes, BMI, and glucose.

Gradient boosting

Gradient Boosting is an ensemble learning technique that sequentially builds a series of weak learners, typically shallow decision trees, each correcting the errors of its predecessor, ultimately creating a strong predictive model by combining their outputs. It minimizes a predefined loss function by iteratively fitting new models to the residuals of the combined predictions. This model also aims to predict the TenYearCHD outcome based on the same set of predictors as the logistic regression model. 

### Part 6. interpret the parameters of your method, if applicable. if no parameters, interpret the contribution of the features have to the model.

The random forests provide measures of feature importance, which indicate how much each feature contributes to the prediction. If a feature like age has a high importance score, it suggests that age is a strong predictor in determining the risk of coronary heart disease across all the sequential builded Trees.
For other features such as currentSmoker or BMI, their importance scores reflect their relative contribution to the model's predictions. A lower score would indicate that the feature is less influential.
