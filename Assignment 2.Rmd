---
title: "SLV Assignment 2"
author: "Hidde, Ilya, Parisa, & Pim"
date: '2023-09-21'
output: html_document
---

# Prediction Model

```{r setup, include=FALSE,comment=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(mice)
library(ggplot2)
library(corrplot)
library(randomForest)
library(caret)
```

### Part 1. Introduction

```{r, include=FALSE}
data <- read.csv("heartdisease.csv")
data %>% head
```

World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. Half the deaths in the United States are due to cardio vascular diseases. The early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. [This research](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression) intends to pinpoint the most relevant/risk factors of heart disease. The data includes:

• Sex: male or female(Nominal)
• Age: Age of the patient;(Continuous)
• Current Smoker: whether or not the patient is a current smoker (Nominal)
• Cigs Per Day: the number of cigarettes that the person smoked on average in one day
• BP Meds: whether or not the patient was on blood pressure medication (Nominal)
• Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)
• Prevalent Hyp: whether or not the patient was hypertensive (Nominal)
• Diabetes: whether or not the patient had diabetes (Nominal)
• Tot Chol: total cholesterol level (Continuous)
• Sys BP: systolic blood pressure (Continuous)
• Dia BP: diastolic blood pressure (Continuous)
• BMI: Body Mass Index (Continuous)
• Heart Rate: heart rate (Continuous)
• Glucose: glucose level (Continuous)
• 10 year risk of coronary heart disease CHD (binary: “1”, means “Yes”, “0” means “No”) 

This assignment is structured as follows: In the first part, we present our data. In part 2, we tidy, explore and describe our data, making it possible to further process it. In part 3, we provide basic prediction models, supported by the explanations, interpretations, and graphs. In part 4, We improve our models by making them more complex, which is also accompanied with a discussion. In the part 5, conclusions about predictions are given

### Part 2. Data wrangling
#### Data Type

Lets explore format of each column.

```{r}
str(data)
```

Not all columns were imported as data types that we expected but, so we need to change the type for some of the variables.

```{r}
#specifying factor variables
dataupd <- data %>%
mutate(male = as.factor(male),
       currentSmoker = as.factor(currentSmoker),
       education = as.factor(education),
       BPMeds = as.factor(BPMeds),
       prevalentStroke = as.factor(prevalentStroke),
       prevalentHyp = as.factor(prevalentHyp),
       diabetes = as.factor( diabetes),
       TenYearCHD = as.factor(TenYearCHD))
```

Now, they are specified in a more relevant manner.

#### Missing data
Since factors are specified correctly we are able to explore missing data patterns:
```{r}
md.pattern(dataupd)
```
The md.pattern shows that there are not many missing values in this dataset. PMeds, education, and glucose are the variables which have absent values the most often. It generally can be ignored since the observations in general are usually complete. We will, however, fill in missing values by mean imputation. Later, data can be restored in a different way. 

Investigate missing values

We look since this variable has the most missing variables. We look at the distribution of the variable and the distribution of the variable when the missing values are removed. 

```{r}
R <- is.na(dataupd$glucose) 

histogram(~male|R, data=dataupd)
histogram(~age|R, data=dataupd)
histogram(~education|R, data=dataupd)
histogram(~cigsPerDay|R, data=dataupd)
```
There don't seem to be any differences in the distribution of the variables when the missing values are removed. Therefore, we can assume that the missing values are missing at random or missing completely at random. 


Imputing for sensitivity analysis
```{r}
imp <- mice(dataupd, m = 5, maxit = 20,  seed = 123, print = F)
plot(imp)
imp$meth
stripplot(imp)
```



```{r}
dataupd <- complete(imp)
```

#### normalize data

To simplify model interpretation.

```{r}
#specify function
normalise <- function(x) (x-min(x))/(max(x)-min(x))

dataupd <- dataupd %>%
  mutate_if(is.integer, as.numeric) %>%
  mutate_if(is.numeric, normalise)
```


```{r}
# Maybe deleting all values is better than mean imputation, as your underestimating the uncertainty, does not preserve the variability in the data.
dataupd_nomissingvalues <- na.omit(dataupd)
```

```{r}
# This is with z score
# Define a function to standardize (z-score normalize) a numeric vector
standardize <- function(x) {
  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)
}

# Apply z-score standardization to all numeric columns in dataupd_nomissingvalues
dataupd_nomissingvalues <- dataupd_nomissingvalues %>%
  mutate_if(is.numeric, standardize)
```


### Part 3. Basic Predictive Modelling

```{r}
mod1 <- glm(TenYearCHD ~ male + age + currentSmoker + diabetes + BMI + glucose, data = dataupd, family = "binomial")
summary(mod1)

mod1_mice <- with(imp, glm(TenYearCHD ~ male + age + currentSmoker + diabetes + BMI + glucose, data = dataupd, family = "binomial"))
pool.mod1_mice <- pool(mod1_mice)
summary(pool.mod1_mice)
```

```{r}
# Set seed for reproducibility
set.seed(123)

# Train 80% and test 20%
train_proportion <- 0.8

# Create an index for splitting the data
train_index <- createDataPartition(dataupd_nomissingvalues$TenYearCHD, p = train_proportion, list = FALSE)

# Split the data into training and test sets
train_data <- dataupd_nomissingvalues[train_index, ]
test_data <- dataupd_nomissingvalues[-train_index, ]

# For the training dataset (train_data)
train_predictors <- train_data[, c("male", "age", "currentSmoker", "diabetes", "BMI", "glucose")]
train_target <- train_data$TenYearCHD

# For the test dataset (test_data)
test_predictors <- test_data[, c("male", "age", "currentSmoker", "diabetes", "BMI", "glucose")]
test_target <- test_data$TenYearCHD
```

```{r}
# Cross-validation
cv <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Create the logistic regression model with cross-validation
model_logistic <- train(
  x = train_predictors, 
  y = train_target, 
  method = "glm", 
  family = "binomial",
  trControl = cv
)

# Print the model results
print(model_logistic)

# Extract and print the coefficients
coef_summary <- summary(model_logistic$finalModel)
print(coef_summary)
```

```{r}
# Make predictions on the test data using the trained logistic regression model
test_predictions <- predict(model_logistic, newdata = test_predictors)

# Confusion matrix test
test_confusion_matrix <- confusionMatrix(test_predictions, test_target)

# Confusion matrix on the test data
print(test_confusion_matrix)
```







### Part 4. More complex model

```{r}
# Cross-validation
cv <- trainControl(method = "cv", number = 5)  # 5-fold cross-validation

# Create the random forest model with cross-validation
model_rf <- train(
  x = train_predictors, 
  y = train_target, 
  method = "rf",  # Specify "rf" for random forest
  trControl = cv
)

# Print the model results
print(model_rf)

# Extract and print the coefficients
coef_summary <- summary(model_rf$finalModel)
print(coef_summary)
```

```{r}
# Make predictions on the test data using the trained random forest model
test_predictions_rf <- predict(model_rf, newdata = test_predictors)

# Confusion matrix random forest
test_confusion_matrix_rf <- confusionMatrix(test_predictions_rf, test_target)

# Print the confusion matrix for the random forest model on the test data
print(test_confusion_matrix_rf)
```


